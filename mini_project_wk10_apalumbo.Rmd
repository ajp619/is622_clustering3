---
title: "Clustering with Hadoop and Spark"
author: "Aaron Palumbo"
date: "October 30, 2015"
output: html_document
---

For our clustering experiments, we will be using the Birch3 dataset from http://cs.joensuu.fi/sipu/datasets/. This is a simple 2-D dataset of 100000 observations and 100 clusters. This simple dataset will allow us to concentrate on the algorithms and give us an easy way to visualize the results. As a baseline, we can use the builtin kmeans.

```{r}
birch3 <- read.table("birch3.txt")
km <- kmeans(birch3, 100)

plot(birch3)
points(km$centers, pch=19, col="red")
```

We can see from the above plot that there is some room for improvement.

Let's start with Hadoop.

## Load Data into Spark

```{r}
# initialte Spark
sc <- SparkR.init()
# Create an sql environment - not exactly sure why we need this
sqlContext <- sparkRSQL.init(sc)
# get data into spark
sdf <- createDataFrame(sqlContext, birch3)
```

## Initialize clusters

One method of doing this would be to find the k points that are furthest from each other. This makes a lot of  sense to me, but I'm not sure how to do this with Spark (or with this many data). Another method would be to initialize the clusters randomly. This we can do:

```{r}
k <- 100
seeds <- collect(sample(sdf, FALSE, k / nrow(sdf)))
```


Let's take a look at what we got:

```{r}
plot(birch3)
points(seeds, pch=19, col="red")
```

It's a start, but it doesn't really captue the clusters very well.

Moving on ...








